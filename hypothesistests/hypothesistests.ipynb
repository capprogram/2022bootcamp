{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Tests for Correlations and Distributions\n",
    "\n",
    "This tutorial introduces hypothesis tests, which test the significance of a result (for example, that two data sets are correlated, or that two data sets are drawn from different populations) by quantifying the probability of obtaining the same result by chance under the _null hypothesis_ (for example, that the data sets are uncorrelated, or that the data sets are drawn from the same population). Before completing this tutorial, you should have had an introduction to basic statistics including uncertainties and confidence intervals, as in the slides and tutorials [here](https://github.com/capprogram/2021bootcamp/wiki). \n",
    "\n",
    "Author: Sheila Kannappan\n",
    "Last Modified: June 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# ipython \"magic\" to enable static plot output directly to notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Tests\n",
    "\n",
    "Correlation tests are a special case of hypothesis tests, which:\n",
    "\n",
    "* Need not involve a model; may be “non-parametric”\n",
    "* Return the probability of the null hypothesis\n",
    "\n",
    "For correlation tests, the null hypothesis is that the two data sets have no association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson vs. spearman rank [and kendall tau] correlation tests\n",
    "data=np.loadtxt(\"anscombe.txt\")\n",
    "# four data sets all of which have linear fits y = 3.00 + 0.500x\n",
    "# and nearly identical statistics (mean, sigma, linear correlation coeff.)\n",
    "# used to illustrate the importance of LOOKING AT YOUR DATA\n",
    "x1=data[:,0]\n",
    "y1=data[:,1]\n",
    "x2=data[:,2]\n",
    "y2=data[:,3]\n",
    "x3=data[:,4]\n",
    "y3=data[:,5]\n",
    "x4=data[:,6]\n",
    "y4=data[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard data set\n",
    "plt.plot(x1, y1,'g.',markersize=10)\n",
    "testxvals=np.array([3.,7.,11.,15.,19.]) # use to make line\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y1-(3.+0.5*x1))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "# notice how the \"plt.text\" command works\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curved data set\n",
    "plt.plot(x2, y2,'g.',markersize=10)\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y2-(3.+0.5*x2))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('curved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad outlier data set\n",
    "plt.plot(x3, y3,'g.',markersize=10)\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y3-(3.+0.5*x3))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('outlier')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage data set\n",
    "plt.plot(x4, y4,'g.',markersize=10)\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y4-(3.+0.5*x4))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('garbage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sigma symbol as a string for use on plots\n",
    "sigmasym=r'$\\sigma$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1, y1,'g.',markersize=10)\n",
    "testxvals=np.array([3.,7.,11.,15.,19.]) # use to make line\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y1-(3.+0.5*x1))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "# notice how the \"plt.text\" command works\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('standard')\n",
    "plt.ylabel('y')\n",
    "\n",
    "print(\" \")\n",
    "print(\"Standard:\")\n",
    "\n",
    "# Spearman rank correlation test\n",
    "cc,pnull=stats.spearmanr(x1,y1)\n",
    "# pnull is returned as a 2-sided p-value by spearmanr, pearsonr, kendalltau\n",
    "# print info to screen\n",
    "print(\"Spearman rank correlation coefficient %f\" % cc)\n",
    "print(\"Spearman rank probability of no correlation %f\" % pnull)\n",
    "# convert pnull to equivalent confidence expressed as # sigma for Gaussian\n",
    "confidence=stats.norm.interval(1-pnull) # fill in with enclosed prob. (0 to 1)\n",
    "# note that by default \"interval\" assumes a Gaussian of mean 0 and sigma 1\n",
    "# returns 2-sided upper & lower c.i. bounds\n",
    "# add expression of confidence as # sigma to plot at position (x,y)=(8.5,3)\n",
    "leveltext='Spearman rank %0.1f' % confidence[1]\n",
    "plt.text(8.5,3,leveltext+sigmasym, size=11, color='b')\n",
    "\n",
    "# Pearson correlation test\n",
    "cc,pnull=stats.pearsonr(x1,y1)\n",
    "print(\"Pearson correlation coefficient %f\" % cc)\n",
    "print(\"Pearson probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Pearson %0.1f' % confidence[1]\n",
    "plt.text(8.5,4,leveltext+sigmasym, size=11, color='b')\n",
    "\n",
    "# now try adding stats.kendalltau by analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-panel plotting (see http://matplotlib.org/users/pyplot_tutorial.html)\n",
    "plt.figure(1,figsize=(12, 8))\n",
    "plt.clf()\n",
    "\n",
    "# standard data set\n",
    "ax1=plt.subplot(221) # 2 rows x 2 columns of plots, 1st plot (top left)\n",
    "plt.plot(x1, y1,'g.',markersize=10)\n",
    "testxvals=np.array([3.,7.,11.,15.,19.]) # use to make line\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y1-(3.+0.5*x1))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "# notice how the \"plt.text\" command works\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('standard')\n",
    "plt.ylabel('y')\n",
    "plt.setp(ax1.get_xticklabels(), visible=False) # hide its xlabels\n",
    "\n",
    "print(\" \")\n",
    "print(\"Standard:\")\n",
    "# Spearman rank correlation test\n",
    "cc,pnull=stats.spearmanr(x1,y1)\n",
    "print(\"Spearman rank correlation coefficient %f\" % cc)\n",
    "print(\"Spearman rank probability of no correlation %f\" % pnull)\n",
    "# convert pnull to equivalent confidence expressed as # sigma for Gaussian\n",
    "confidence=stats.norm.interval(1-pnull) # fill in with enclosed prob. (0 to 1)\n",
    "# note that by default \"interval\" assumes a Gaussian of mean 0 and sigma 1\n",
    "# returns 2-sided upper & lower c.i. bounds\n",
    "# add expression of confidence as # sigma to plot at position (x,y)=(8.5,3)\n",
    "leveltext='Spearman rank %0.1f' % confidence[1]\n",
    "plt.text(8.5,3,leveltext+sigmasym, size=11, color='b')\n",
    "# Pearson correlation test\n",
    "cc,pnull=stats.pearsonr(x1,y1)\n",
    "print(\"Pearson correlation coefficient %f\" % cc)\n",
    "print(\"Pearson probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Pearson %0.1f' % confidence[1]\n",
    "plt.text(8.5,4,leveltext+sigmasym, size=11, color='b')\n",
    "# Kendall tau correlation test\n",
    "cc,pnull=stats.kendalltau(x1,y1)\n",
    "print(\"Kendall tau correlation coefficient %f\" % cc)\n",
    "print(\"Kendall tau probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Kendall tau %0.1f' % confidence[1]\n",
    "plt.text(8.5,5,leveltext+sigmasym, size=11, color='b')\n",
    "\n",
    "# curved data set\n",
    "ax2=plt.subplot(222) # 2 rows x 2 columns of plots, 2nd plot (top right)\n",
    "plt.plot(x2, y2,'g.',markersize=10)\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y2-(3.+0.5*x2))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('curved')\n",
    "plt.setp(ax2.get_xticklabels(), visible=False) # hide its xlabels\n",
    "plt.setp(ax2.get_yticklabels(), visible=False) # hide its ylabels\n",
    "\n",
    "print(\" \")\n",
    "print(\"Curved:\")\n",
    "# Spearman rank correlation test\n",
    "cc,pnull=stats.spearmanr(x2,y2)\n",
    "print(\"Spearman rank correlation coefficient %f\" % cc)\n",
    "print(\"Spearman rank probability of no correlation %f\" % pnull)\n",
    "# convert pnull to equivalent confidence expressed as # sigma for Gaussian\n",
    "confidence=stats.norm.interval(1-pnull) # fill in with enclosed prob. (0 to 1)\n",
    "# note that by default \"interval\" assumes a Gaussian of mean 0 and sigma 1\n",
    "# returns 2-sided upper & lower c.i. bounds\n",
    "# add expression of confidence as # sigma to plot at position (x,y)=(8.5,3)\n",
    "leveltext='Spearman rank %0.1f' % confidence[1]\n",
    "plt.text(8.5,3,leveltext+sigmasym, size=11, color='b')\n",
    "# Pearson correlation test\n",
    "cc,pnull=stats.pearsonr(x2,y2)\n",
    "print(\"Pearson correlation coefficient %f\" % cc)\n",
    "print(\"Pearson probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Pearson %0.1f' % confidence[1]\n",
    "plt.text(8.5,4,leveltext+sigmasym, size=11, color='b')\n",
    "# Kendall tau correlation test\n",
    "cc,pnull=stats.kendalltau(x2,y2)\n",
    "print(\"Kendall tau correlation coefficient %f\" % cc)\n",
    "print(\"Kendall tau probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Kendall tau %0.1f' % confidence[1]\n",
    "plt.text(8.5,5,leveltext+sigmasym, size=11, color='b')\n",
    "\n",
    "# bad outlier data set\n",
    "ax3=plt.subplot(223) # 2 rows x 2 columns of plots, 3rd plot (bottom left)\n",
    "plt.plot(x3, y3,'g.',markersize=10)\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y3-(3.+0.5*x3))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('outlier')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "print(\" \")\n",
    "print(\"Outlier:\")\n",
    "# Spearman rank correlation test\n",
    "cc,pnull=stats.spearmanr(x3,y3)\n",
    "print(\"Spearman rank correlation coefficient %f\" % cc)\n",
    "print(\"Spearman rank probability of no correlation %f\" % pnull)\n",
    "# convert pnull to equivalent confidence expressed as # sigma for Gaussian\n",
    "confidence=stats.norm.interval(1-pnull) # fill in with enclosed prob. (0 to 1)\n",
    "# note that by default \"interval\" assumes a Gaussian of mean 0 and sigma 1\n",
    "# returns 2-sided upper & lower c.i. bounds\n",
    "# add expression of confidence as # sigma to plot at position (x,y)=(8.5,3)\n",
    "leveltext='Spearman rank %0.1f' % confidence[1]\n",
    "plt.text(8.5,3,leveltext+sigmasym, size=11, color='b')\n",
    "# Pearson correlation test\n",
    "cc,pnull=stats.pearsonr(x3,y3)\n",
    "print(\"Pearson correlation coefficient %f\" % cc)\n",
    "print(\"Pearson probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Pearson %0.1f' % confidence[1]\n",
    "plt.text(8.5,4,leveltext+sigmasym, size=11, color='b')\n",
    "# Kendall tau correlation test\n",
    "cc,pnull=stats.kendalltau(x3,y3)\n",
    "print(\"Kendall tau correlation coefficient %f\" % cc)\n",
    "print(\"Kendall tau probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Kendall tau %0.1f' % confidence[1]\n",
    "plt.text(8.5,5,leveltext+sigmasym, size=11, color='b')\n",
    "\n",
    "# garbage data set\n",
    "ax4=plt.subplot(224)\n",
    "plt.plot(x4, y4,'g.',markersize=10)\n",
    "plt.plot(testxvals,3.+0.5*testxvals,'r',linestyle=':',linewidth=2.)\n",
    "rms=np.sqrt(np.mean((y4-(3.+0.5*x4))**2))\n",
    "plt.text(3,12,'rms %0.2f' % rms,size=11,color='b')\n",
    "plt.xlim(2,20)\n",
    "plt.ylim(2,14)\n",
    "plt.title('garbage')\n",
    "plt.xlabel('x')\n",
    "plt.setp(ax4.get_yticklabels(), visible=False) # hide its ylabels\n",
    "\n",
    "print(\" \")\n",
    "print(\"Garbage:\")\n",
    "# Spearman rank correlation test\n",
    "cc,pnull=stats.spearmanr(x4,y4)\n",
    "print(\"Spearman rank correlation coefficient %f\" % cc)\n",
    "print(\"Spearman rank probability of no correlation %f\" % pnull)\n",
    "# convert pnull to equivalent confidence expressed as # sigma for Gaussian\n",
    "confidence=stats.norm.interval(1-pnull) # fill in with enclosed prob. (0 to 1)\n",
    "# note that by default \"interval\" assumes a Gaussian of mean 0 and sigma 1\n",
    "# returns 2-sided upper & lower c.i. bounds\n",
    "# add expression of confidence as # sigma to plot at position (x,y)=(8.5,3)\n",
    "leveltext='Spearman rank %0.1f' % confidence[1]\n",
    "plt.text(8.5,3,leveltext+sigmasym, size=11, color='b')\n",
    "# Pearson correlation test\n",
    "cc,pnull=stats.pearsonr(x4,y4)\n",
    "print(\"Pearson correlation coefficient %f\" % cc)\n",
    "print(\"Pearson probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Pearson %0.1f' % confidence[1]\n",
    "plt.text(8.5,4,leveltext+sigmasym, size=11, color='b')\n",
    "# Kendall tau correlation test\n",
    "cc,pnull=stats.kendalltau(x4,y4)\n",
    "print(\"Kendall tau correlation coefficient %f\" % cc)\n",
    "print(\"Kendall tau probability of no correlation %f\" % pnull)\n",
    "confidence=stats.norm.interval(1-pnull)\n",
    "leveltext='Kendall tau %0.1f' % confidence[1]\n",
    "plt.text(8.5,5,leveltext+sigmasym, size=11, color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wrap\n",
    "\n",
    "Now that you've seen the plots, take a look at the companion slides on correlations [here](https://github.com/capprogram/2021bootcamp/blob/master/BasicStatsII.pdf).\n",
    "\n",
    "Also, google and/or discuss the [p-value crisis](https://en.wikipedia.org/wiki/Misuse_of_p-values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some data\n",
    "data = np.genfromtxt(\"ECO_dr1_subset.csv\", delimiter=\",\", dtype=None, names=True, encoding=None)\n",
    "name = data['NAME']\n",
    "logmstar = data ['LOGMSTAR']\n",
    "urcolor = data['MODELU_RCORR']\n",
    "cz = data['CZ']\n",
    "goodur = (urcolor > -99) & (logmstar > 10.)\n",
    "colors=urcolor[goodur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms vs. KDE Plots\n",
    "\n",
    "Here's a nice figure from Ivezic et al. \"Data Mining and Machine Learning in Astronomy\" to illustrate the difference between histograms and Kernel Density Estimation (KDE) plots. KDE plots represent the sum of a lot of little shapes, each located at the position of one data value. The shapes are called \"kernels\" and their width is the band width or kernel width.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/capprogram/2021bootcamp/master/hypothesistests/IvezicFig.6.1.jpg'/>\n",
    "\n",
    "Notice that KDE plots (even with a rectangular kernel) are superior to histograms in that they don't have arbitrary bin edges (which can affect the appearance of the data, compare top two panels). The optimal kernel width can be computed using elaborate statistical techniques, but a good first guess is the Knuth bin width that is used as an optimal histogram bin width (balancing blurring information against presenting spurious levels of detail) -- google \"Knuth's rule\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plots are much nicer than histograms\n",
    "### This cell might print out a depreciation warning afterwards, just re-run it to get rid of it ###\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "xx = np.linspace(-1,4,10000)[:,np.newaxis]\n",
    "bw1=0.05\n",
    "bw2=0.05\n",
    "kde1 = KernelDensity(kernel='epanechnikov',bandwidth=bw1).fit(colors[:,np.newaxis])\n",
    "logdens1 = kde1.score_samples(xx)\n",
    "kde2 = KernelDensity(kernel='gaussian',bandwidth=bw2).fit(colors[:,np.newaxis])\n",
    "logdens2 = kde2.score_samples(xx)\n",
    "plt.plot(xx,np.exp(logdens1),color='red',label='kde1')\n",
    "plt.plot(xx,np.exp(logdens2),color='blue',alpha=0.5,label='kde2')\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now comparing different binwidths for same kernel, vs. above compared different kernels for same binwidth\n",
    "bwopt=0.09\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "xx = np.linspace(-1,4,10000)[:,np.newaxis]\n",
    "bw1=0.05\n",
    "bw2=bwopt \n",
    "kde1 = KernelDensity(kernel='gaussian',bandwidth=bw1).fit(colors[:,np.newaxis])\n",
    "logdens1 = kde1.score_samples(xx)\n",
    "kde2 = KernelDensity(kernel='gaussian',bandwidth=bw2).fit(colors[:,np.newaxis])\n",
    "logdens2 = kde2.score_samples(xx)\n",
    "plt.plot(xx,np.exp(logdens1),color='red',label='kde1')\n",
    "plt.plot(xx,np.exp(logdens2),color='blue',alpha=0.5,label='bwopt')\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare color distributions for galaxies at closer and further distances\n",
    "nearby = (cz[goodur] < 5500.) # redshift is a proxy for distance\n",
    "selenvnear = np.where(nearby)\n",
    "selenvfar = np.where(~nearby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.xlim(0,3)\n",
    "bw = bwopt\n",
    "kde = KernelDensity(kernel='epanechnikov',bandwidth=bw).fit(colors[selenvnear][:,np.newaxis])\n",
    "logdens = kde.score_samples(xx)\n",
    "plt.plot(xx,np.exp(logdens),'r--',label=\"near\")\n",
    "kde = KernelDensity(kernel='epanechnikov',bandwidth=bw).fit(colors[selenvfar][:,np.newaxis])\n",
    "logdens = kde.score_samples(xx)\n",
    "plt.plot(xx,np.exp(logdens),'b--',label=\"far\")\n",
    "plt.xlabel(\"u-r color (mag)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Tests\n",
    "The Mann-Whitney U test is mostly sensitive to differences between the distribution medians, while the Kolmogorov-Smirnov test is more sensitive to shape/spread differences. Note that the K-S test cannot be used for samples with repeated values. Perhaps obviously, these tests should never be used to compare a model with the data used to determine that model's parameters. They are most useful to compare two different data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Kolmogorov-Smirnov and Mann-Whitney U tests to compare distributions\n",
    "DD, pnullks = stats.ks_2samp(colors[selenvnear],colors[selenvfar])\n",
    "UU, pnullmw = stats.mannwhitneyu(colors[selenvnear],colors[selenvfar])\n",
    "print(\"K-S pnull = %0.2g\" % pnullks)\n",
    "print(\"M-W pnull = %0.2g\" % pnullmw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yikes!\n",
    "These samples don't seem to be drawn from the same distribution. Any idea why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of dividing by distance, divide randomly\n",
    "namegood = name[goodur]\n",
    "\n",
    "sample2inds = np.random.choice(len(namegood),size=int(round(0.5*len(namegood)-1)),replace=False) #note replace=False!\n",
    "flag12 = np.zeros(len(namegood),dtype=int)\n",
    "flag12[sample2inds] = 1\n",
    "flag12 += 1\n",
    "    \n",
    "sample1inds = np.where(flag12 == 1)\n",
    "sample2inds = np.where(flag12 == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.clf()\n",
    "# for histograms, can use Freedman binwidth as 'fd' (similar to Knuth's rule, not quite as optimal)\n",
    "n, bins, patches = plt.hist(colors[sample1inds],bins='fd',label='1',histtype='stepfilled',color='red',alpha=0.25)\n",
    "plt.hist(colors[sample2inds],bins=bins,label='2',histtype='stepfilled',color='blue',alpha=0.25)\n",
    "plt.xlim(0,3)\n",
    "DD, pnullks = stats.ks_2samp(colors[sample1inds],colors[sample2inds])\n",
    "plt.text(1.1, 200, \"K-S pnull = %0.2g\" % pnullks, size=14, color='b')\n",
    "plt.xlabel(\"u-r color (mag)\")\n",
    "plt.legend()\n",
    "\n",
    "# rerunning the randomization in the previous cell then this plot over and over should give you a sense of how much\n",
    "# pnull can jump around -- remember the p-value crisis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why dividing by distance didn't work\n",
    "It wasn't the distance itself -- the ECO survey is volume-limited with a luminosity floor, so it has no inherent bias with distance. Also the colors are rest-frame, k-corrected, etc. However, cosmic variance in environments causes different large-scale structure in the near and far parts of the volume, which leads to changes in the relative number of red-sequence vs. blue-sequence galaxies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
